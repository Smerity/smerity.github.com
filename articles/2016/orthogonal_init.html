<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="always">
    <title>Smerity.com: Explaining and illustrating orthogonal initialization for recurrent neural networks</title>
    
  <link rel="shortcut icon" href="/media/images/favicon.ico">

    
  
  <meta property="og:title" content="Explaining and illustrating orthogonal initialization for recurrent neural networks">
  
  <meta name="description" content="One of the most extreme issues with recurrent neural networks (RNNs) are vanishing and exploding gradients. Whilst there are many methods to combat this, such as gradient clipping for exploding gradients and more complicated architectures including the LSTM and GRU for vanishing gradients, orthogonal initialization is an interesting yet simple approach." />
  <meta property="og:description" content="One of the most extreme issues with recurrent neural networks (RNNs) are vanishing and exploding gradients. Whilst there are many methods to combat this, such as gradient clipping for exploding gradients and more complicated architectures including the LSTM and GRU for vanishing gradients, orthogonal initialization is an interesting yet simple approach." />
  <meta name="twitter:description" content="One of the most extreme issues with recurrent neural networks (RNNs) are vanishing and exploding gradients. Whilst there are many methods to combat this, such as gradient clipping for exploding gradients and more complicated architectures including the LSTM and GRU for vanishing gradients, orthogonal initialization is an interesting yet simple approach." />
  <!-- Seems excessive, doesn't it? -->
  
  
  <meta property="og:image" content="http://smerity.com/media/images/articles/2016/orthogonal.png" />
  <meta name="twitter:image:src" content="http://smerity.com/media/images/articles/2016/orthogonal.png" />
  <meta name="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Smerity.com" />
  <meta property="og:type" content="article" />
  <meta name="twitter:site" content="@Smerity" />
  <meta name="twitter:creator" content="Smerity" />
  

    
  <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
  <!--[if lt IE 9]>
    <script src="/media/js/html5.js"></script>
  <![endif]-->

  
  <link rel="stylesheet" type="text/css" href="/media/compiled_less/bootstrap.css">

  <link rel="stylesheet" type="text/css" href="/media/css/pygments.css">

    
  <script type="text/javascript" src="/media/js/jquery.js"></script>
  <script src="/media/bootstrap/js/bootstrap.js"></script>
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: false,
          skipTags: ["script","noscript","style","textarea","code"]
        }
      });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
  </script>


    
    
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-470775-5']);
    _gaq.push(['_trackPageview']);
    setTimeout("_gaq.push(['_trackEvent', '15_seconds', 'read'])",15000);
    setTimeout("_gaq.push(['_trackEvent', '30_seconds', 'read'])",30000);
    setTimeout("_gaq.push(['_trackEvent', '60_seconds', 'read'])",60000);
    setTimeout("_gaq.push(['_trackEvent', '120_seconds', 'read'])",120000);
    setTimeout("_gaq.push(['_trackEvent', '240_seconds', 'read'])",240000);
    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

  </head>
  <body>
  


  
  
<div class="hero-box">
  <div class="hero-inner min-padded">
    <a href="/">
      <h3 class="heavy pull-left">&laquo; Smerity.com</h3>
    </a>
    <h3 class="maroon contact-me pull-right">
      <!-- All on one line seems crazy but it prevents small form factors trying to fit them over multiple lines -->
      <a href="https://www.twitter.com/Smerity/" alt="Twitter"><i class="icon-twitter"></i></a><a href="https://facebook.com/smerity" alt="Facebook"><i class="icon-facebook-sign"></i></a><a href="https://github.com/Smerity/" alt="GitHub"><i class="icon-github"></i></a><a href="http://au.linkedin.com/in/smerity" alt="LinkedIn"><i class="icon-linkedin"></i></a><a href="mailto:smerity@smerity.com" alt="Email"><i class="icon-envelope-alt"></i></a>
    </h3>
  </div>
</div>

  <div class="container">
    
      <div class="row">
      <div class="span8 content-wrapper ">
        
  

        <div class="content-box">
        
<h1 class="post-title">Explaining and illustrating orthogonal initialization for recurrent neural networks</h1>
<h3 class="post-date">June 27, 2016</h3>
  <!--
The flow of gradients is one of the most important aspects when it comes to training neural networks.
The gradient ... specified by the error gives information to the rest of the network.
Unfortunately, they're also one of the most finicky aspects of neural networks as well.
Many parameters, such as weight initialization, choice of activation function, and network depth, all have an impact.
In this blog post, we'll explore the impact of some of those decisions.

To help solidify our understanding of vanishing and exploding gradients, we'll explore a small number of examples.
While this will serve as an introduction to these issues, these issues are best understood by your own experimentation.
Even toy tasks can provide an opportunity to visualize and better understand the implications for larger tasks.
-->

<!--
Upon receiving each element of an input stream \\(x_1, x_2, \dots, x_t, \dots\\), having started from an initial state \\(h_0\\), an RNN updates its internal state \\(h_t = f(x_t, h_{t-1})\\) and producing an output \\(y_t = g(h_t)\\).
-->

<p>One of the most extreme issues with recurrent neural networks (RNNs) are vanishing and exploding gradients.
Whilst there are many methods to combat this, such as gradient clipping for exploding gradients and more complicated architectures including the LSTM and GRU for vanishing gradients, orthogonal initialization is an interesting yet simple approach.</p>
<p>To help solidify our understanding of why orthogonal initialization works, we'll explore it in terms of basic linear algebra and stability theory.</p>
<p><center><video controls autoplay loop>
  <source src="/media/images/articles/2016/eigenvalue_orthogonal.m4v" type="video/mp4">
  Your browser does not support the video tag.
</video></center></p>
<h2>Eigenvalues, stability theory, and repeated matrix multiplication</h2>
<p>A fundamental action performed in deep learning is repeated matrix multiplications.
These repeated matrix multiplications mean the resulting matrix is exponential in the number of layers of the neural network.
This can have serious numerical stability issues, with the result exploding or disappearing quickly.
One of the worst culprits in terms of this are RNNs which repeatedly update an internal state using a single weight matrix, sometimes over hundreds or thousands of timesteps.</p>
<p>Stability theory asks what the dynamics of a system are after a long period of time and can be used to gain insights here.
While stability theory covers many methods, we're particularly interested in how eigenvalues can be used to quickly compute repeated matrix multiplication.</p>
<p>Let's take a step away from RNNs for a second to establish some fundamentals.</p>
<h3>Repeated matrix multiplication and Fibonacci</h3>
<p>Imagine you wanted to compute the Fibonacci sequence using matrices.
It turns out there exists a matrix \(F\) that, when you perform repeated matrix multiplication on it, you can use to iteratively compute the Fibonacci sequence.</p>
<script type="math/tex; mode=display">
F^n = \begin{pmatrix}
0 & 1 \\
1 & 1 \\
\end{pmatrix} ^ n
= \begin{pmatrix}
F^{n-1} & F^{n} \\
F^{n} & F^{n+1} \\
\end{pmatrix}
</script>

<p>Special note, a matrix to the power of 0 gives you the identity matrix, placing zeros in the \(F^{n}\) entries of the matrix, giving the correct solution of \(F^0 = 0\).</p>
<p>Let's compute the 4th entry in the Fibonacci sequence using this method, starting at identity and then performing a matrix multiplication with \(F\) at each step.
Notice that the intermediate values on the diagonals are all part of the Fibonacci sequence - 0, 1, 1, 2, 3.</p>
<script type="math/tex; mode=display">
\begin{align*}
\begin{pmatrix}
0 & 1 \\1 & 1\end{pmatrix} ^ 4 & = \begin{pmatrix}1 & 0 \\0 & 1\end{pmatrix} \begin{pmatrix}0 & 1 \\1 & 1\end{pmatrix} ^ 4 \\
& = \begin{pmatrix}0 & 1 \\1 & 1\end{pmatrix} \begin{pmatrix}0 & 1 \\1 & 1\end{pmatrix} ^ 3 \\
& = \begin{pmatrix}1 & 1 \\1 & 2\end{pmatrix} \begin{pmatrix}1 & 1 \\1 & 0\end{pmatrix} ^ 2 \\
& = \begin{pmatrix}1 & 2 \\2 & 3\end{pmatrix} \begin{pmatrix}1 & 1 \\1 & 0\end{pmatrix} ^ 1 \\
& = \begin{pmatrix}2 & 3 \\3 & 5\end{pmatrix}
\end{align*}
</script>

<p>Expanded out like this, it's also clear we could calculate \(F^4\) by computing \((F^2)^2\) instead of \(F^4\).
This allows us to compute the nth entry in the Fibonacci sequence in \(O(\log n)\) time.</p>
<p>We can go one step better however, computing the nth entry in the Fibonacci sequence in \(O(1)\) time (assuming constant time pow() function) whilst also gaining some intuition as to what happens as it grows.</p>
<h3>Eigenvalues</h3>
<p>Time to dust off some elementary linear algebra.
An eigendecomposition of a matrix represents the matrix in terms of its eigenvalues and eigenvectors.
The matrix \(F\) above can be factorized as \(F = Q \Lambda Q^{-1}\), where \(\Lambda\) is a diagonal matrix with the eigenvalues placed on the diagonals and \(Q\) is a matrix composed of the corresponding eigenvectors of \(F\).</p>
<script type="math/tex; mode=display">
Q = \begin{pmatrix}-0.8506 & -0.5257 \\0.5257 & -0.8506\end{pmatrix} \\
\Lambda = \begin{pmatrix}-0.6180 & 0 \\0 & 1.6180\end{pmatrix} \\
Q^{-1} = \begin{pmatrix}-0.8506 & 0.5257 \\-0.5257 & -0.8506\end{pmatrix}
</script>

<p>An immediate and sane question to ask is why bother representing the matrix this way?
Let's use this new matrix to compute the next Fibonacci value.</p>
<script type="math/tex; mode=display">
\begin{align*}
F^2 & = (Q \Lambda Q^{-1}) (Q \Lambda Q^{-1}) \\
& = Q \Lambda (Q^{-1} Q) \Lambda Q^{-1} \\
& = Q \Lambda ^ 2 Q^{-1} \\
\end{align*}
</script>

<p>Other than the two constant matrix multiplications, the only extra work you need to do is raising the \(\Lambda\) matrix to the nth power!
This holds true for any power you might want to apply, such that \(F^n = Q \Lambda ^ n Q^{-1}\).</p>
<p>What this simply means is that the eigenvalues are what dictate the growth or death of the result as we perform repeated matrix multiplication.</p>
<p>Using the Fibonacci matrix above, notice that one eigenvalue explodes whilst the other eigenvalue vanishes as we get to larger and larger \(n\) for \(\Lambda^n\).
Even at \(n = 10\), we find that one of the eigenvalues is almost irrelevant.</p>
<script type="math/tex; mode=display">
\Lambda^{10} = \begin{pmatrix}0.0081 & 0 \\0 & 122.9919\end{pmatrix} \\
</script>

<p>Indeed, if you just multiply a reasonably large Fibonacci number by the largest eigenvalue, you get essentially the next Fibonacci value.
It's no surprise that the largest eigenvalue happens to be the Golden ratio, \(\varphi\), or the converged ratio of consecutive Fibonacci numbers.</p>
<div class="codehilite"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">13</span> <span class="o">*</span> <span class="mf">1.618</span> <span class="o">**</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">89</span><span class="p">]</span>
</pre></div>


<p>The summary is that, as \(n\) gets asymptotically large:</p>
<ul>
<li>The matrix \(F^n\) vanishes if the absolute value of all eigenvalues are smaller than 1</li>
<li>The matrix \(F^n\) maintains a constant norm if the absolute value of all eigenvalues are 1</li>
<li>The matrix \(F^n\) explodes if the absolute value of any eigenvalue is larger than 1</li>
</ul>
<h2>Orthogonal Initialization</h2>
<h3>Orthogonal matrices review</h3>
<p>Orthogonal matrices have many interesting properties but the most important for us is that all the eigenvalues of an orthogonal matrix have absolute value 1.
This means that, no matter how many times we perform repeated matrix multiplication, the resulting matrix doesn't explode or vanish.</p>
<p>It's interesting to note what the constraint that an eigenvalue must have absolute value 1 means.
If we are only using real numbers, that means the eigenvalues must be either +1 or -1, resulting in very boring matrices.
We can extend to complex eigenvalues however, allowing for far more interesting results when repeatedly multiplied.</p>
<p>Even if the eigenvalues are complex, they can still result in matrices that are all real.
The simplest example of this is the 2x2 <a href="https://en.wikipedia.org/wiki/Rotation_matrix">rotation matrix</a>.
Whilst the rotation matrix \(R\) below (which performs a 90 degree rotation) is a real matrix, the eigenvalues and eigenvectors are complex.</p>
<script type="math/tex; mode=display">
R = \begin{pmatrix}
0 & 1 \\
-1 & 0 \\
\end{pmatrix} \\
\Lambda = \begin{pmatrix}
1i & 0 \\
0 & -1i \\
\end{pmatrix} \\
Q = \begin{pmatrix}
0.707 & 0.707 \\
0.707i & -0.707i \\
\end{pmatrix}
</script>

<!--
Following the above, we can use \\(R^0\\) (identity) or \\(R^2\\) to rotate 180 degrees.
As you might expect, to not rotate at all, we can use \\(R^0\\) (identity), and if we want to rotate 180 degrees, we can use \\(R^2\\).

    :::python
    >>> # R is a 90 degree rotation matrix
    >>> R = np.matrix([[0, 1], [-1, 0]])
    >>> eval, evec = np.linalg.eig(R)
    >>> Notice the eigenvalues and eigenvectors are complex
    >>> eval
    array([ 0.+1.j,  0.-1.j])
    >>> evec
    matrix([[ 0.70710678+0.j        ,  0.70710678-0.j        ],
            [ 0.00000000+0.70710678j,  0.00000000-0.70710678j]])
    >>> # Yet they reconstruct the real rotation matrix R from above (minus minor floating point issues) using eigendecomposition
    >>> evec.dot(np.diag(eval)).dot(np.linalg.inv(evec))
    matrix([[ 0.+0.j,  1.+0.j],
            [-1.+0.j,  0.+0.j]])
    >>> # Identity - 0 degree rotation
    >>> evec.dot(np.diag(eval ** 0)).dot(np.linalg.inv(evec))
    matrix([[ 1.+0.j,  0.+0.j],
            [ 0.+0.j,  1.+0.j]])
    >>> # 180 degree rotation (i.e. R ** 2)
    >>> evec.dot(np.diag(eval ** 2)).dot(np.linalg.inv(evec))
    matrix([[-1.+0.j,  0.+0.j],
            [ 0.+0.j, -1.+0.j]])
-->

<!--A special note is that the eigenvalues can be complex, with the complex part of the eigenvalue contributing an oscillatory component.-->

<!--
There has also been explorations into using the unitary matrix for RNN initialization as well, though there are additional complications given the unitary matrix is a complex analogue of the orthogonal matrix.
-->

<h2>Orthogonal init for RNNs</h2>
<p>For this illustration, we're only looking at a simplified RNN model.
For simplicity we assume no inputs, no bias, an identity activation function \(f\), and the initial hidden state of the RNN \(h_0\) is an identity matrix.</p>
<script type="math/tex; mode=display">
h_t = f(W h_{t-1} + V x_t) = f(W h_{t-1}) = W h_{t-1} \\
h_3 = W ( W (W h_0) ) = W^3 h_0 = W^3 I = W^3
</script>

<p>When we perform repeated matrix multiplication within an RNN and the result explodes or vanishes, we also land in the realm of vanishing or exploding gradients.
If the gradients vanish, training is at a near standstill as no information is backpropagated.
If the gradients explode, training may never converge as the gradient update wildly fluctuates.
Both these are hugely problematic and can occur in a very small number of timesteps.</p>
<p>When initializing the weight matrix in RNNs, it's not uncommon to use random uniform or random normal initialization.
This method gives no guarantee as to the eigenvalues of the weight matrix and are likely to result in exploding or vanishing results.</p>
<p>If we instead use an orthogonal matrix to initialize the weights, the resulting matrix neither explodes nor vanishes.
This allows for gradients to backpropagate more effectively.</p>
<p>To construct an orthogonal matrix, we can use singular value decomposition, as seen in the <a href="https://github.com/Lasagne/Lasagne/blob/a3d44a7fbb84b1206a3959881c52b2203f48fc44/lasagne/init.py#L363">Lasagne init.py</a>.  <br />
</p>
<h2>Visualizing different inits</h2>
<p>Let's see what happens to the resulting matrix when performing repeated matrix multiplication over 64 timesteps.
Remember that it's also not uncommon for RNNs to run for hundreds or even thousands of timesteps.
Neural networks also usually have additional complications such as a non-linear activation function and varying input.</p>
<h3>Vanishing matrix due to small eigenvalues</h3>
<p><center><video controls autoplay loop>
  <source src="/media/images/articles/2016/eigenvalue_vanish.m4v" type="video/mp4">
  Your browser does not support the video tag.
</video></center></p>
<h3>Exploding matrix due to an eigenvalue &gt; 1</h3>
<p><center><video controls autoplay loop>
  <source src="/media/images/articles/2016/eigenvalue_explode.m4v" type="video/mp4">
  Your browser does not support the video tag.
</video></center></p>
<h3>Orthogonal matrix</h3>
<p><center><video controls autoplay loop>
  <source src="/media/images/articles/2016/eigenvalue_orthogonal.m4v" type="video/mp4">
  Your browser does not support the video tag.
</video></center></p>
<h2>Conclusion</h2>
<p>Orthogonal initialization is a simple yet relatively effective way of combatting exploding and vanishing gradients, especially when paired with other methods such as gradient clipping and more advanced architectures.
While we haven't gone into detail with mathematical derivation of the gradients, it is clear when visualizing orthogonal initialization why it can be so effective.
The story becomes far more complicated when we add in real activation functions and input however.</p>
<p>If you're interested in more detail, I highly recommend the papers below which all explore various aspects of this.</p>
<h2>What to look at next</h2>
<ul>
<li><a href="https://arxiv.org/abs/1312.6120">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a> - "... Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. ... We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos."</li>
<li><a href="http://arxiv.org/abs/1211.5063">On the difficulty of training Recurrent Neural Networks</a> - "There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). ... Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem."</li>
<li><a href="http://arxiv.org/abs/1511.08400">Regularizing RNNs by Stabilizing Activations</a> - "In the absence of inputs and nonlinearities, a constant norm would imply orthogonality of the hidden-to-hidden
transition matrix for simple RNNs (SRNNs). However, in the case of an orthogonal transition
matrix, inputs and nonlinearities can still change the norm of the hidden state, resulting in
instability. This makes targeting the hidden activations directly a more attractive option for achieving
norm stability. Stability becomes especially important when we seek to generalize to longer
sequences at test time than those seen during training (the 'training horizon')."</li>
<li><a href="http://arxiv.org/abs/1511.06464">Unitary Evolution Recurrent Neural Networks</a> - "When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1."</li>
<li><a href="http://arxiv.org/abs/1602.06662">Recurrent Orthogonal Networks and Long-Memory Tasks</a> - "In this work we analyze these “long-memory” tasks, and construct explicit RNN solutions for them. The solutions illuminate both the tasks, and provide a theoretical justification for the success of recent approaches using orthogonal initializations of, or unitary constraints on, the transition matrix of the RNN. ... We verify experimentally that initializing correctly (i.e. random orthogonal or identity) is critical for success on these tasks."</li>
<li><a href="http://arxiv.org/abs/1602.02218">Strongly-Typed Recurrent Neural Networks</a></li>
</ul>

        </div>
      </div>
      
        <div class="span3 side-box">
          <div style="padding: 8px;">
            <h1 style="margin-bottom: 0; text-align: center;">Popular Articles</h1>
            <style>
            ul.unstyled li { padding-bottom: 6px; }
            </style>
            <ul class="crossed">
              <li><a target="_blank" href="/articles/2016/algorithms_can_be_prejudiced.html">It's ML, not magic: machine learning can be prejudiced</a></li>
              <li><a target="_blank" href="/articles/2016/ml_not_magic.html">It's ML, not magic: the rise of AI-prefix investing</a></li>
              <li><a target="_blank" href="/articles/2016/architectures_are_the_new_feature_engineering.html">In deep learning, architecture engineering is the new feature engineering</a></li>
              <!--<li><a target="_blank" href="/articles/2015/keras_qa.html">Question answering on the Facebook bAbi dataset using recurrent neural networks and 175 lines of Python + Keras</a></li>-->
              <li><a target="_blank" href="/articles/2015/google_sparsehash.html">How Google Sparsehash achieves two bits of overhead per entry using sparsetable</a></li>
              <li><a target="_blank" href="/articles/2013/where_did_all_the_http_referrers_go.html">Where did all the HTTP referrers go?</a></li>
            </ul>

            <p class="center-text">
              <b>Interested in saying hi? ^_^</b><br />
              <a href="https://twitter.com/Smerity" class="twitter-follow-button" data-size="large" data-show-count="false">Follow @Smerity</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
              <div class="contact-me">
              <a href="https://www.twitter.com/Smerity/" alt="Twitter"><i class="icon-twitter"></i></a><a href="https://facebook.com/smerity" alt="Facebook"><i class="icon-facebook-sign"></i></a><a href="https://github.com/Smerity/" alt="GitHub"><i class="icon-github"></i></a><a href="http://au.linkedin.com/in/smerity" alt="LinkedIn"><i class="icon-linkedin"></i></a><a href="mailto:smerity@smerity.com" alt="Email"><i class="icon-envelope-alt"></i></a>
              </div>
            </p>

            <div class="well well-small">
              <!--<h1 style="margin-bottom: 0; margin-top: 0; text-align: center;">Hi, I'm Smerity</h1>-->
              <div align="center">
                <!--img class="shadowed pronounced" src="/media/images/smerity_bw.jpg" />-->
              </div>
              <!-- TODO: Place Twitter here -->
              <p>
                I'm <b>Stephen Merity</b>, better known in most places as <b>Smerity</b>.
              </p>
              <p>
                <b>Salesforce Research:</b><br />
                Senior research scientist<br />(deep learning)
              </p>
              <p>
                <b>Part of MetaMind:</b><br />
                Acquired by Salesforce
              </p>
              <p>
                <b>Harvard University:</b><br />
                <a href="http://www.seas.harvard.edu/programs/graduate/computational-science-and-engineering/master-of-science-in-cse">MS in CSE</a>
              </p>
              <p>
                <b>Sydney University:</b><br />
                <a href="http://sydney.edu.au/engineering/it/current_students/undergrad/bit.shtml">BIT</a> (University Medal + First Class Honours)
              </p>
              <p>
                <b><a href="/abme.html">Full about me</a></b>
              </p>
            </div>

            
          </div>
        </div>
      
      </div>
    
    
<footer>
  You reached the bottom! Reading can be challenging, so I think you deserve a reward. I'd offer you <a href="http://www.minecraftwiki.net/wiki/Cake">cake</a>, but after <a href="http://en.wikipedia.org/wiki/Portal_(video_game)">a certain robot-human conflict</a> started due to cake it may not be a wise choice. So here:
  <div align="center">
    <img src="/media/images/mini_coin.png" height="32" width="32" />
  </div>
  Take a coin. No, not a Bitcoin. Do you know how <a title="When I originally put this note here, Bitcoins were only a few dollars each...">expensive</a> those are? ಠ_ಠ
</footer>

  </div>
  
  
  <link rel="stylesheet" type="text/css" href="/media/css/fontawesome/fontawesome.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" type="text/css" href="/media/css/fontawesome/fontawesome-ie7.css" />
  <![endif]-->
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,700' rel='stylesheet' type='text/css'>

  
  </body>
</html>