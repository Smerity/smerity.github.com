<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="always">
    <title>Smerity.com: In deep learning, architecture engineering is the new feature engineering</title>
    
  <link rel="shortcut icon" href="/media/images/favicon.ico">

    
  
  <meta property="og:title" content="In deep learning, architecture engineering is the new feature engineering">
  
  <meta name="description" content="Whilst deep learning has simplified feature engineering, it certainly hasn&#39;t removed it. Feature engineering is now hidden in architecture engineering." />
  <meta property="og:description" content="Whilst deep learning has simplified feature engineering, it certainly hasn&#39;t removed it. Feature engineering is now hidden in architecture engineering." />
  <meta name="twitter:description" content="Whilst deep learning has simplified feature engineering, it certainly hasn&#39;t removed it. Feature engineering is now hidden in architecture engineering." />
  <!-- Seems excessive, doesn't it? -->
  
  
  <meta property="og:image" content="http://smerity.com/media/images/articles/2016/imagenet_conv_kernels.png" />
  <meta name="twitter:image:src" content="http://smerity.com/media/images/articles/2016/imagenet_conv_kernels.png" />
  <meta name="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Smerity.com" />
  <meta property="og:type" content="article" />
  <meta name="twitter:site" content="@Smerity" />
  <meta name="twitter:creator" content="Smerity" />
  

    
  <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
  <!--[if lt IE 9]>
    <script src="/media/js/html5.js"></script>
  <![endif]-->

  
  <link rel="stylesheet" type="text/css" href="/media/compiled_less/bootstrap.css">

  <link rel="stylesheet" type="text/css" href="/media/css/pygments.css">

    
  <script type="text/javascript" src="/media/js/jquery.js"></script>
  <script src="/media/bootstrap/js/bootstrap.js"></script>

    
    
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-470775-5']);
    _gaq.push(['_trackPageview']);
    setTimeout("_gaq.push(['_trackEvent', '15_seconds', 'read'])",15000);
    setTimeout("_gaq.push(['_trackEvent', '30_seconds', 'read'])",30000);
    setTimeout("_gaq.push(['_trackEvent', '60_seconds', 'read'])",60000);
    setTimeout("_gaq.push(['_trackEvent', '120_seconds', 'read'])",120000);
    setTimeout("_gaq.push(['_trackEvent', '240_seconds', 'read'])",240000);
    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

  </head>
  <body>
  


  
  
<div class="hero-box">
  <div class="hero-inner min-padded">
    <a href="/">
      <h3 class="heavy pull-left">&laquo; Smerity.com</h3>
    </a>
    <h3 class="maroon contact-me pull-right">
      <!-- All on one line seems crazy but it prevents small form factors trying to fit them over multiple lines -->
      <a href="https://www.twitter.com/Smerity/" alt="Twitter"><i class="icon-twitter"></i></a><a href="https://facebook.com/smerity" alt="Facebook"><i class="icon-facebook-sign"></i></a><a href="https://github.com/Smerity/" alt="GitHub"><i class="icon-github"></i></a><a href="http://au.linkedin.com/in/smerity" alt="LinkedIn"><i class="icon-linkedin"></i></a><a href="mailto:smerity@smerity.com" alt="Email"><i class="icon-envelope-alt"></i></a>
    </h3>
  </div>
</div>

  <div class="container">
    
      <div class="row">
      <div class="span8 content-wrapper">
        
  

        <div class="content-box">
        
<h1 class="post-title">In deep learning, architecture engineering is the new feature engineering</h1>
<h3 class="post-date">June 11, 2016</h3>
  <p>Two of the most important aspects of machine learning models are <a href="https://en.wikipedia.org/wiki/Feature_extraction">feature extraction</a> and <a href="https://en.wikipedia.org/wiki/Feature_engineering">feature engineering</a>.
Those features are what supply relevant information to the machine learning models.</p>
<hr />
<p>Representing the word <strong>overfitting</strong> using various feature representations:</p>
<ul>
<li>Morphological = [(prefix, <strong>over-</strong>), (root, <strong>fit</strong>), (suffix=imperfect tense, <strong>-ing</strong>)]<br />
</li>
<li>Unigrams = ['o', 'v', 'e', 'r', 'f', 'i', 't', 't', 'i', 'n', 'g']</li>
<li>Bigrams = ['ov', 've', 'er', 'rf', 'fi', 'it', 'tt', 'ti', 'in', 'ng']</li>
<li>Trigrams = ['ove', 'ver', 'erf', 'rfi', 'fit', 'itt', 'tti', 'tin', 'ing']</li>
<li>One-hot = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]<br />
</li>
<li><a href="https://en.wikipedia.org/wiki/Word_embedding">Word vector</a> = [-0.26, 0.34, 0.48, -0.06, 0.16, 0.11, 0.13, -0.15, 0.47, -0.49, 0.07, -0.39, -0.13, -0.15, 0.06, 0.09]</li>
<li>...</li>
</ul>
<hr />
<p>If the features are few or irrelevant, your model may have a hard time making any useful predictions.
If there are too many features, your model will be slow and likely overfit.
<!--
In general, better features produce better models.
Producing features by hand is a difficult (frequently requiring human experts with domain knowledge) and expensive (in terms of human effort) endeavour.
It also assumes that a human being knows what a machine learning model requires to make the best decisions.
-->
<!--
Even if we know how to produce a useful feature, we might skip it as it may require large amounts of human effort or large amounts of computation time.
--></p>
<!--In an optimal world, the machine learning model would learn what features it needs as part of the learning process.-->

<p>Humans don't necessarily know what feature representation are best for a given task.
Even if they do, relying on feature engineering means that a human is always in the loop.
This is a far cry from the future we might want, where you can throw any dataset at a machine learning system and have it produce insights without human help.</p>
<h2>The promise of deep learning</h2>
<p>The romanticized description of deep learning usually promises that the days of hand crafted feature engineering are gone - that the models are advanced enough to work this out themselves.
Like most advertising, this is simultaneously true and misleading.</p>
<p>Whilst deep learning has simplified feature engineering in many cases, it certainly hasn't removed it.
As feature engineering has decreased, the architectures of the machine learning models themselves have become increasingly more complex.
Most of the time, these model architectures are as specific to a given task as feature engineering used to be.</p>
<!--
> Feature engineering is now hidden in the architecture.
-->

<p>To clarify, this is still an important step.
Architecture engineering is more general than feature engineering and provides many new opportunities.
Having said that, however, we shouldn't be oblivious to the fact that where we are is still far from where we intended to be.</p>
<!-- 
> Any time a human being forces a specific architectural decision on a machine learning model, we're restricting the flexibility of the model.

> If there doesn't exist a model that could learn those architectural decisions, we're essentially hand coding a feature.
-->

<blockquote>
<p>Any time a human being forces an architectural decision that couldn't be learned, we're essentially <em>hard coding</em> a feature.</p>
</blockquote>
<h2>The Convolutional Neural Network (CNN)</h2>
<p>A major reason for the resurgence in popularity of neural networks were their impressive results from the ImageNet contest in 2012.
The model produced and <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">documented</a> by Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.
That's a staggeringly large improvement - and the model did allow the removal of many hand crafted features too!</p>
<p>CNNs are a prime example of the promise of deep learning - removing complicated and problematic hand crafted feature engineering.
Edge detection is no longer handled by an explicitly coded human program but is instead learned by the first convolutional layer.
Indeed, the filters learned by the first layer of CNNs when given images are highly reminiscent of <a href="https://en.wikipedia.org/wiki/Gabor_filter">Gabor filters</a>, traditionally used for edge and texture detection.</p>
<hr />
<p><img class="center" src="/media/images/articles/2016/imagenet_conv_kernels.png" /></p>
<p>An example of the convolutional kernels learned in the first layer of the model from <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>.</p>
<hr />
<p>As we get towards higher and higher layers, the filters start recognizing specific patterns and objects.
All of these are learned feature extractions, with the features become further refined as we go up the model's hierarchy.
That's the <em>good</em> part.</p>
<h3>How are CNNs hard coded?</h3>
<blockquote>
<p>CNNs are feature engineering that enforce 2D locality and weight re-usability.
<em>None of that is learned.</em></p>
</blockquote>
<p>Humans are the ones injecting the relationship between pixels (and later CNN neurons) into the machine learning model.
<strong>There is no way for the model to learn these itself.</strong></p>
<p>At no point can the neural network itself say "I think I need the convolution to be larger" or "Sharing my weights with everyone else doesn't make any sense in this area of the image - this area is always text whilst the other areas are always real world images".</p>
<blockquote>
<p>You might not consider this an issue for 2D images - but what if a human doesn't know the relationship between certain parameters in a complex input space?</p>
</blockquote>
<hr />
<p><a target="_blank" href="/media/images/articles/2016/dota2_cm_orig.jpg">
<img class="center" src="/media/images/articles/2016/dota2_cm.jpg" />
</a></p>
<p>As a simple example, imagine we wanted to extract information from a game.
In DotA 2, as with most games, you're provided additional context via an in-game <a href="http://dota2.gamepedia.com/Head-up_display">heads-up display</a> on the top and bottom with the middle containing a "viewport" into the game's world.
Re-using weights between all these elements would at best be wasteful and at worst result in a poor representation.
The mini-map in the bottom left would likely benefit from a very small convolutional window size compared to the viewport.
Teaching all potential convolutional windows to be able to read the numbers for XP / HP / gold / level / move speed / etc would likely be a massive waste.<br />
And this is a <strong>game</strong>. In the grand scheme of things, it's "not complex".</p>
<hr />
<p>Whilst it would be possible to learn 2D locality by beginning with a fully dense layer, this would be far slower and require larger amounts of training data to get comparable results.
Initially, each neuron would be connected to all other neurons in the previous layer.
Given enough training data, the model may learn to zero out the majority of the connections, especially if those neurons are spatially distant.
You may want to help "nudge" the model in this direction by using L1 regularization to enforce sparsity - though it'd be worth remembering that you just injected external knowledge again.</p>
<p>There is no standard way for allowing a machine learning model to take advantage of weight re-use though.
This is a major problem given that weight re-use is a major component of why CNNs work so well.
Each filter is shared across all the visual field, allowing a feature to be detected regardless of the position in the field, even if it has never been seen at that location before.
On top of decreasing the number of parameters that need to be learned, this weight re-use allows the model to achieve better generalization.</p>
<h2>Why is this an issue?</h2>
<p>All of these hand coded architectural modifications are enforced, not learned.
This directly limits the generality of the models.</p>
<p>In the case of images, you may argue that this is fine - we're only injecting a minor amount of knowledge.
Fair enough for images maybe - but what about tasks where humans can't see such "obvious" relations however?
If a machine learning model can't divine the "obvious" relations within a 2D image, do you trust it to reason over more complex input where a human dare not tread?</p>
<p>In an optimal world, a meta machine learning algorithm would explore and decide upon the model features and model architecture best suited to the task at hand.
This would happen with little to no human intervention.</p>
<p>This is not an optimal world however.
We generally have to settle for either human made or badly brute forced.
There are people trying to improve our ability to "search" over the theoretical model architecture space - but even then progress is limited.</p>
<p>(Note: I'm not providing a full literature review in the follow two sections, only providing some interesting jumping points for interested explorers - I omit many notable publications such as the NeuroEvolution of Augmented Topologies (NEAT) algorithm and others)</p>
<h3>Searching for recurrent neural network architectures</h3>
<p>Long ago, in 1994, Angeline et al. produced <a href="https://www.semanticscholar.org/paper/An-evolutionary-algorithm-that-constructs-Angeline-Saunders/ba45ce377a073f0e954a517f56b96ef66af0a5d7?citingPapersSort=is-influential&amp;citedPapersSort=is-influential">An evolutionary algorithm that constructs recurrent neural networks</a>.
They tried to move away from genetic algorithms towards evolutionary algorithms for searching architectures.</p>
<p>More recently, in <a href="https://www.semanticscholar.org/paper/An-Empirical-Exploration-of-Recurrent-Network-J%C3%B3zefowicz-Zaremba/324fc9c732116fa81624faad07524039f193cede">An Empirical Exploration of Recurrent Network Architectures</a>, Jozefowicz et al. perform a thorough architecture search where they evaluated over ten thousand different RNN architectures.
They were motivated to explore whether the human hand crafted LSTM, one of the most frequently used of RNN architectures, was actually optimal.
Whilst they produced interesting insights, the approach is not likely scalable to more general architectural modifications.</p>
<h3>Searching for convolutional neural network architectures</h3>
<p>Very recently, indeed just after I'd ranted that CNNs provided enforced constraints on the model, Fernando et al. from Google DeepMind released <a href="http://mlanctot.info/files/papers/gecco16-dppn.pdf">Convolution by Evolution</a>.
They evolve / train a neural network to rediscover an (approximate) convolutional neural network structure with a fully connected (read: dense) layer.
This means it not only learns about 2D locality but it also learns how to share / compress weights.
Whilst exciting, their technique has only been done over relatively toy datasets so far however.</p>
<h2>Far from utopia</h2>
<p>We're still far away from a world where machine learning models can live without their human tutors.
If you investigate the state of the art model architectures for given tasks, you'll inevitably find instances of specific architectural engineering in almost every one of them.
Those changes will generally be human specified and integral to achieving state of the art performance...</p>
<blockquote>
<p>If we tailor a model to each and every task, do we really have anything generic?</p>
</blockquote>
<p>Surely there's a better way?</p>
<h3>Addendum:</h3>
<ul>
<li><a href="https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d">ResNets</a> - one of the most exciting recent techniques - is explicitly working around the inability of deep neural networks to just perform the identity function (i.e. nothing) when it's of their best interest!</li>
<li>Attention models are allowing a form of iterative analysis over the data and provide fascinating insights (such as <a href="https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/071b16f25117fb6133480c6259227d54fc2a5ea0">discovering word alignment for machine translation with no help from humans</a>) but could not be constructed (at least so far) by any of our architectural search procedures</li>
</ul>

        </div>
      </div>
      
        <div class="span3 side-box">
          <div style="padding: 8px;">
            <h1 style="margin-bottom: 0; text-align: center;">Popular Articles</h1>
            <style>
            ul.unstyled li { padding-bottom: 6px; }
            </style>
            <ul class="crossed">
              <li><a href="/articles/2015/keras_qa.html">Question answering on the Facebook bAbi dataset using recurrent neural networks and 175 lines of Python + Keras</a></li>
              <li><a href="/articles/2015/google_sparsehash.html">How Google Sparsehash achieves two bits of overhead per entry using sparsetable</a></li>
              <li><a href="/articles/2013/where_did_all_the_http_referrers_go.html">Where did all the HTTP referrers go?</a></li>
            </ul>
            <div class="well well-small">
              <!--<h1 style="margin-bottom: 0; margin-top: 0; text-align: center;">Hi, I'm Smerity</h1>-->
              <div align="center">
                <!--img class="shadowed pronounced" src="/media/images/smerity_bw.jpg" />-->
              </div>
              <!-- TODO: Place Twitter here -->
              <p>
                I'm <b>Stephen Merity</b>, better known in most places as <b>Smerity</b>.
              </p>
              <p>
                <b>Sydney University:</b><br />
                <a href="http://sydney.edu.au/engineering/it/current_students/undergrad/bit.shtml">BIT</a> (University Medal + First Class Honours)
              </p>
              <p>
                <b>Harvard University:</b><br />
                <a href="http://www.seas.harvard.edu/programs/graduate/computational-science-and-engineering/master-of-science-in-cse">MS in CSE</a>
              </p>
              <p>
                <b><a href="/abme.html">Read more about me</a></b>
              </p>
            </div>
            <p class="center-text">
              <b>Interested in saying hi?</b><br />
              <div class="contact-me">
              <a href="https://www.twitter.com/Smerity/" alt="Twitter"><i class="icon-twitter"></i></a><a href="https://facebook.com/smerity" alt="Facebook"><i class="icon-facebook-sign"></i></a><a href="https://github.com/Smerity/" alt="GitHub"><i class="icon-github"></i></a><a href="http://au.linkedin.com/in/smerity" alt="LinkedIn"><i class="icon-linkedin"></i></a><a href="mailto:smerity@smerity.com" alt="Email"><i class="icon-envelope-alt"></i></a>
              </div>
            </p>

            
          </div>
        </div>
      
      </div>
    
    
<footer>
  You reached the bottom! Reading can be challenging, so I think you deserve a reward. I'd offer you <a href="http://www.minecraftwiki.net/wiki/Cake">cake</a>, but after <a href="http://en.wikipedia.org/wiki/Portal_(video_game)">a certain robot-human conflict</a> started due to cake it may not be a wise choice. So here:
  <div align="center">
    <img src="/media/images/mini_coin.png" height="32" width="32" />
  </div>
  Take a coin. No, not a Bitcoin. Do you know how <a title="When I originally put this note here, Bitcoins were only a few dollars each...">expensive</a> those are? ಠ_ಠ
</footer>

  </div>
  
  
  <link rel="stylesheet" type="text/css" href="/media/css/fontawesome/fontawesome.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" type="text/css" href="/media/css/fontawesome/fontawesome-ie7.css" />
  <![endif]-->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,700' rel='stylesheet' type='text/css'>

  
  </body>
</html>