<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="always">
    <title>Smerity.com: It&#39;s ML, not magic: simple questions you should ask to help reduce AI hype</title>
    
  <link rel="shortcut icon" href="/media/images/favicon.ico">

    
  
  <meta property="og:title" content="It&#39;s ML, not magic: simple questions you should ask to help reduce AI hype">
  
  <meta name="description" content="AI is a young field full of amazing potential. This mystery and lack of understanding allows for hype to grow unchecked." />
  <meta property="og:description" content="AI is a young field full of amazing potential. This mystery and lack of understanding allows for hype to grow unchecked." />
  <meta name="twitter:description" content="AI is a young field full of amazing potential. This mystery and lack of understanding allows for hype to grow unchecked." />
  <!-- Seems excessive, doesn't it? -->
  
  
  <meta property="og:image" content="http://smerity.com/media/images/articles/2016/ml_not_magic.jpg" />
  <meta name="twitter:image:src" content="http://smerity.com/media/images/articles/2016/ml_not_magic.jpg" />
  <meta name="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Smerity.com" />
  <meta property="og:type" content="article" />
  <meta name="twitter:site" content="@Smerity" />
  <meta name="twitter:creator" content="Smerity" />
  

    
  <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
  <!--[if lt IE 9]>
    <script src="/media/js/html5.js"></script>
  <![endif]-->

  
  <link rel="stylesheet" type="text/css" href="/media/compiled_less/bootstrap.css">

  <link rel="stylesheet" type="text/css" href="/media/css/pygments.css">

    
  <script type="text/javascript" src="/media/js/jquery.js"></script>
  <script src="/media/bootstrap/js/bootstrap.js"></script>
  <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: false,
          skipTags: ["script","noscript","style","textarea","code"]
        }
      });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
  </script>


    
    
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-470775-5']);
    _gaq.push(['_trackPageview']);
    setTimeout("_gaq.push(['_trackEvent', '15_seconds', 'read'])",15000);
    setTimeout("_gaq.push(['_trackEvent', '30_seconds', 'read'])",30000);
    setTimeout("_gaq.push(['_trackEvent', '60_seconds', 'read'])",60000);
    setTimeout("_gaq.push(['_trackEvent', '120_seconds', 'read'])",120000);
    setTimeout("_gaq.push(['_trackEvent', '240_seconds', 'read'])",240000);
    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

  </head>
  <body>
  


  
  
<div class="hero-box">
  <div class="hero-inner min-padded">
    <a href="/">
      <h3 class="heavy pull-left">&laquo; Smerity.com</h3>
    </a>
    <h3 class="maroon contact-me pull-right">
      <!-- All on one line seems crazy but it prevents small form factors trying to fit them over multiple lines -->
      <a href="https://www.twitter.com/Smerity/" alt="Twitter"><i class="icon-twitter"></i></a><a href="https://facebook.com/smerity" alt="Facebook"><i class="icon-facebook-sign"></i></a><a href="https://github.com/Smerity/" alt="GitHub"><i class="icon-github"></i></a><a href="http://au.linkedin.com/in/smerity" alt="LinkedIn"><i class="icon-linkedin"></i></a><a href="mailto:smerity@smerity.com" alt="Email"><i class="icon-envelope-alt"></i></a>
    </h3>
  </div>
</div>

  <div class="container">
    
      <div class="row">
      <div class="span8 content-wrapper">
        
  

        <div class="content-box">
        
<h1 class="post-title">It&#39;s ML, not magic: simple questions you should ask to help reduce AI hype</h1>
<h3 class="post-date">July 3, 2016</h3>
  <p>During the peak of the <a href="https://en.wikipedia.org/wiki/Dot-com_bubble">dot-com bubble</a>, you'd be forgiven for thinking <a href="https://www.techdirt.com/articles/20031204/0824235.shtml">prefix investing</a> was a legitimate tactic.
A company could receive a nice jump in valuation by adding an "e-" prefix or ".com" suffix.
Just being awake to the potential of the World Wide Web was enough to indicate to investors that a company might take advantage of it.
What many of those suffixes and prefixes missed however was a detailed plan of attack.</p>
<p>The internet was young and full of promises that were either technically or logistically impossible to fulfill.
Some of the promises made were irrationally optimistic.
Others were downright fraudulent.
Even if what was promised could become a reality, few companies had the talent or foresight to execute a well thought out internet strategy.</p>
<p>In the years since, after the rise and fall of the dot-com bubble, we have seen the promise of the internet play out in full, transforming the way we live and communicate.
Many of the possibilities that became a reality years later were invisible to even the most keen of observers at that early stage.
The bubble didn't doom the eventual rise of internet technology - but it did make life far more complex.</p>
<h2>Enter, stage left: Artificial Intelligence</h2>
<p>If there's any promise I can make about the field of AI, it's that the hype will always overtake the research.
In this article, I won't quibble over definitions, simply taking the broadest term as used in the media: artificial intelligence is whenever a system appears more intelligent than we expect it to be.
This is in recognition of both <a href="https://en.wikipedia.org/wiki/AI_effect">the AI effect</a> (where well defined applications of AI/ML are no longer considered intelligence) and that even if a system is simply an application of basic statistics, it is likely to be reported as AI if it appears intelligent.
<!--This is apt given that many of the AI-prefix companies (known simply as AI-prefixes from here on) are utilizing that broad definition to their advantage.--></p>
<!--
(My more scientifically minded friends are likely converting their Bunsen burners to flame throwers right now but one issue at a time...)
-->

<p>Like the internet before and after the dot-com bubble, AI is a young technology.
It isn't that AI lacks potential, it's simply that the irrational optimism is overpowering.
Bold (and perhaps impossible to fulfill) promises are a natural result when prefix investing is again profitable - the only difference that the prefix is now "AI-".
As with the dot-com bubble, few entities have the talent or foresight to properly execute on the complex AI strategies that they might have promised themselves into.</p>
<p>AI captures our imagination in a way that routers and ethernet cables can't.
AI slots cleanly into our existing fiction, building on a fear that humans have long held.
<a href="https://en.wikipedia.org/wiki/Frankenstein">Frankenstein</a> engrossed us with tales of unorthodox scientific experiments granting sentience to a corpse.
Further back are <a href="https://en.wikipedia.org/wiki/Golem">golems</a>, anthropomorphic beings magically created entirely from inanimate matter.
<!--These all inspire modern day retellings, except with the flesh of Frankenstein's monster or the clay of the golem being replaced by the steel of SkyNet.-->
These all inspire modern day retellings, except with the flesh and clay replaced by the steel of SkyNet.</p>
<p>The narrative of AI also allows commentators with no experience in the field to make strong statements about the future.
These non-expert commentators are given equal weighting compared to experts who may be stating the opposite.
This is not an <a href="https://yourlogicalfallacyis.com/appeal-to-authority">appeal to authority</a> - it's just a request for evidence.
While experts are by no means infallible, a general requirement of their field is that they provide evidence to back up their claims.
Certain arguments, such as the exponential growth of hardware capability or the concept of self-improving systems, have a tendency to remove any evidence requirements from a conversation.</p>
<!--For whatever reason, it seems we have decided that commentators need little in the way of evidence to back up their bold statements.-->
<!--While we can certainly go too far one direction - experts are by no means infallible - it's likely still a good idea to listen to input from those actually implementing the technology being promised.-->
<!--This could be compared to the traditional manager - programmer relationship, where one side promises and the other delivers.-->

<p>The fundamental and inconvenient truth is that science fiction is more interesting than science fact.
Just as stories and companies reporting advances on medicine or physics should be taken with a grain of salt, <em>we should combat this fictionalization of the reality of AI</em>.
<!--We need to be aware of this as we try our best to combat it.-->
This can be a painful realization for practitioners in the fields of AI/ML/NLP/CV.
For them, there is no need to hype the research - reality is exciting enough.</p>
<!--
It's no surprise that the AI practitioners themselves are frequently those telling you to discount the hype.
The field of AI has already been through this before: the AI winter.
Overpromises have already cost us before.
-->

<p>This post aims to outlay a few simple rules that, without restricting one's ability to "dream big", should be able to cull the most ludicrous examples of AI-prefixing.</p>
<!--
I have a great deal to say on these but this isn't the post for that - this post is focused on the present and near future, where none of these apply.
(n.b. I'd usually refer to it as ML/AI but let's stick with the term that results in the majority of the hype)

The TechDirt article above that 
Directly quoting [the TechDirt article](https://www.techdirt.com/articles/20031204/0824235.shtml) above, originally in relation to the dot-com prefix investing, then used on nanotechnology companies:

> This is a sign that there are a lot of greedy, but unsophisticated investors dumping lots and lots of dough into the market.
-->

<h2>That awkward situation: journalists, investors, and entrepreneurs</h2>
<p>Why are we in this situation in the first place?
At a basic level, humans want a story.
This feeds in to our Frankensteinian roots from earlier.
The more interesting the hook is, the more likely we are to pay attention, and the more likely we are to want to be a part of it.</p>
<p>This base situation becomes even more extreme when we look at the interplay between entrepreneurs, journalists, and investors.
A not unreasonable tactic to impress both investors and journalists is to wow them.
Journalists even have a secondary desire to wow their readers which can be fed directly from that.
As such, the premise of AI-prefix investing can be extended to journalists by seeing the rise of AI-prefix coverage.</p>
<p>Most disturbing to me was this quote, taken with permission, from a conversation I recently had.
An article in a publication featured a young start-up - except the article was pushing hard on the AI angle.
The issue was that (a) their product didn't currently feature any AI and (b) there were no plans for how to implement AI in their product yet.
They told me:</p>
<blockquote>
<p>Sadly it is what [the] media want to hear about and write about. And it is the only way to pitch in a way that they don't go "boring". I didn't start off saying AI but that is the only way we got a break. And I don't think we are going to change their minds in a hurry.</p>
</blockquote>
<p>This may be an extreme example - being able to get press coverage for an AI related product without any AI - but stretching the truth can still occur even if there is AI under the hood.
Journalists can push for the most exciting story possible, being misled (knowingly or not) by those they interview.
The same holds true for investors.</p>
<p>The bar for actual scientific advances is well established from an academic perspective (and there are even those within the field that think it isn't rigorous enough) - journalists and investors should use that bar or be exceedingly wary when accepting anything below it.</p>
<!--+ Entrepreneurs want press coverage on their idea and are willing to stretch the truth
+ Journalists can push for the most exciting story possible, being misled (knowingly or not) by those they interview
+ This is a well known problem for science as it becomes more popular - the initial study becomes exaggerated (assuming there is an initial study)
+ Viewing it in person doesn't guarantee it works - anecdata and baselines - image caption generation overfitting to training data, image analysis being able to rely simply on semantic knowledge
-->

<h2>The questioning nature that we need</h2>
<p><a href="https://twitter.com/jackclarkSF">Jack Clark</a> is "the world's only neural net reporter", holding a post at Bloomberg.
While this tweet is over almost a year old, it's a succinct explanation of why I really enjoy his coverage.</p>
<p><center>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Got a press release for a co that had &quot;unique science&quot; which made it a &quot;leading technology innovator&quot;. No referenced research papers.</p>&mdash; Jack Clark (@jackclarkSF) <a href="https://twitter.com/jackclarkSF/status/623626817577091072">July 21, 2015</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</center></p>
<p>Beyond requiring evidence to filter for AI-prefixes, he also reads research papers for fun.
Obviously we can't expect every journalist, investor, or lay-person to have this level of insight, but are there some broad rules we can fall back on?</p>
<h3>Ask what cases the system will fail on</h3>
<blockquote>
<p>"Revealing the failure cases of your models is a way of avoiding over-hype"<br />
<a href="https://www.cs.ubc.ca/~murphyk/MLbook/">Kevin Murphy</a> at ICML 2015</p>
</blockquote>
<p>No AI system yet developed will magically fit every use case.
If a researcher tells you that a model got state of the art results out of the box, they're either exceedingly lucky or giving you a fairy tale version of reality.
Even if their model is arbitrarily flexible, there are still fundamental limits placed on us by information theory.</p>
<ul>
<li>"How much training data is required?"</li>
<li>"Can this work unsupervised (= without labelling the examples)?"</li>
<li>"Can the system predict out of vocabulary names?" (i.e. Imagine if I said "My friend Rudinyard was mean to me" - many AI systems would never be able to answer "Who was mean to me?" as Rudinyard is out of its vocabulary)</li>
<li>"How much does the accuracy fall as the input story gets longer?"</li>
<li>"How stable is the model's performance over time?"</li>
</ul>
<p>Asking for an upper bound on what is possible is not an unreasonable question.
It should also have a clear answer given it's something that anyone working with or creating the AI system should have already run into.</p>
<!--**Note:** Research has a fun way of doing this.
It's a common occurrence for a paper to provide both good and bad example output from a system.
Just as good exemplar output is usually carefully selected, the bad output can be selected for comedic effect ;)
-->

<h3>The real world rarely looks like a dataset</h3>
<p>Very few datasets are entirely representative of what the task looks like in the real world.
These limitations exist for a variety of reasons.
The datasets we create usually reflect a minor extension on the capacity of existing AI systems.
Just as the tests we give students are rarely reflective of the real world, neither are the tests we give these AI systems.</p>
<p>You may well ask why datasets are only ever incremental extensions on previous datasets - why not jump straight to datasets reflective of the real world (assuming you can actually collect such a dataset).
Philosophically, we are using these tests / experiments as a process of discovering new information, usually in relation to validating a hypothesis.
If you give a test to a system or person when you know they are going to fail, the test yields zero new information.</p>
<!--Just as we wouldn't test a music student on their knowledge of building chords if they were still learning the keys.-->
<!--Just as it would be a bad idea to test a student on riding a motorbike if they didn't know how to ride a bike, our AI datasets are usually one step beyond what is currently possible.-->
<!--
A test is given to evaluate how well a system performs on a given task.
If we already know it will fail, then the test isn't necessary.
-->

<p>As such, good performance on a dataset also doesn't necessarily imply good performance in the real world.
I'd wager that even trivial self-driving car models, given reasonable data, would be able to score in the high 90s if we decided on a reasonable notion of accuracy.
The real stickler is the last few percent.
When we make a mistake at high speed on a road, potentially under questionable conditions, the results can be disastrous.
If the cost of a mistake is high, accuracy really doesn't mean anything, especially over a dataset that might not be representative in the first place.
The question then becomes what the expected number of failure cases are in standard use.
If the impact of these failure cases are minor or they can be caught later by a human, the AI system may still have a positive impact.</p>
<p>Many datasets are also restricted to a particular domain.
As an example, a standard dataset in natural language processing is the Penn Treebank, composed primarily of articles from the Wall Street Journal in 1989.
Let me repeat that: <strong>1989</strong>.
That was the year I was born, the year the Berlin Wall fell, and the year the original Game Boy was released.
Given language changes over time, it shouldn't be a surprise that modern textual datasets can be vastly different to older ones.</p>
<p>Not only is time an issue but the Wall Street Journal is primarily composed of financial articles.
For many tasks, we have achieved incredibly high accuracy over the Penn Treebank.
When we go to use those trained models elsewhere however, such as on "out of domain" datasets such as scientific articles, the performance can plummet.
This is especially true when applied to Twitter.
Tweets might as well be a different language.</p>
<p>Another interesting question is what's the performance of the best baseline?
For some tasks, depressingly simple tactics can get you the majority of accuracy.
For this reason, newer datasets for visual question answering (i.e. you're given an image and asked "What color is the girl's flowers?") include human baselines where <em>the humans never see the image</em>.
This is important as it represents the best a purely textual model of the world can do.
It turns out that a well tuned purely textual machine learning model, which has never seen any of the images it is being asked about, can sometimes beat more complex models that try to actually, y'know, do visual question answering.</p>
<h3>Any claim of advanced research without publication is suspect at best</h3>
<blockquote>
<p>"If you do research in isolation, the quality goes down. Which is why military research sucks."
Yann LeCun at ICML 2015</p>
</blockquote>
<p>The rate of change in the field of AI is such that anyone on the sidelines is at best keeping up.
It is certainly not impossible for an entity to come out of nowhere with an amazing system but not it is far less likely.
It also means they haven't been put through the standard evaluations that academia would have placed on them.
In some cases this is reasonable - there are many AI systems that are useful for real world applications that will likely never receive a paper - but it is a missing element that you should be aware of.</p>
<p>From earlier:</p>
<blockquote>
<p>The bar for actual scientific advances is well established from an academic perspective (and there are even those within the field that think it isn't rigorous enough) - journalists and investors should use that bar or be exceedingly wary when accepting anything below it.</p>
</blockquote>
<p>If you can see a system working in front of you, congrats, but without proper evaluation it's anecdata - we don't know how well it should work (at a minimum see "baselines"), what the failure cases may be, or how frequent they might occur.</p>
<h3>AI doesn't change the base use case or business fundamentals</h3>
<p>AI won't save a broken business plan.
An easy upper bound is asking if the business plan would work with free human labor replacing the automated component.
Achieving human level performance on any real world task is an exceedingly difficult endeavour when moving away from a few simple and well defined tasks.</p>
<p>We can also ask if the application of AI is a value add or fundamentally transformative.
Many of the AI-prefixes only feature AI as a value add, using that as a hook for media or investment.
AI can still be a useful addition in that context but it emphasizes that the underlying business must be viable.</p>
<p>All of this is to say that if the business plan doesn't work with free humans, AI won't save it.</p>
<!--
### The game of telephone applied to research

The [telephone game](https://en.wikipedia.org/wiki/Chinese_whispers) (also known across the word as Chinese whispers / Russian scandal / ...) might sound childish but seems to be a well known phenomenon issue in the adult world too.

> [O]ne person whispers a message to another, which is passed through a line of people until the last player announces the message to the entire group. Errors typically accumulate in the retellings, so the statement announced by the last player differs significantly, and often amusingly, from the one uttered by the first.

Let's go through a theoretical replaying, shall we?

+ Paper: "In our new paper, we achieve state of the art Cloze test results on the Facebook Children's Book Test using our new model ... Improving on previous performance from the XYZ model from Facebook" 
+ *Note: Cloze tests are "fill in the blank" challenges, frequently allowing you to select between multiple-choices - i.e. "The dog barked angrily. The X then bit the postman"*
+ University: "ML dept teaches computers to better model language using children's books from Facebook"
-->

<h2>Conclusion</h2>
<p>AI is a young field full of amazing potential but much of the mystery that surrounds it is unnecessary.
This mystery and lack of understanding allows for hype to grow unchecked.</p>
<p>As Francois Chollet, author of the Keras deep learning library, notes in <a href="http://blog.keras.io/on-the-importance-of-democratizing-artificial-intelligence.html">democratizing Artificial Intelligence</a>: "making deep learning more accessible should be one of our priorities".
Accessibility extends to audiences far beyond academics and engineers - it goes to journalists, investors, and the broader public as well.</p>
<p>We should combat this fictionalization of the reality of AI - and asking the questions above is a good start.</p>
<hr />
<p><strong>Thanks to:</strong></p>
<ul>
<li><a href="http://kylebillings.com/">Kyle Billings</a> for the initial article image - unfortunately that was removed after <a href="http://motherboard.vice.com/read/dallas-pd-using-a-bomb-robot-to-kill-a-suspect-is-an-unprecedented-shift-in-policing">an event the morning of publication</a></li>
<li><a href="https://www.flickr.com/photos/tim_uk/7942941272/">Tim Sheerman-Chase</a> for his Creative Commons licensed image that replaced the image above</li>
<li><a href="https://twitter.com/wejradford">Will Radford</a> for pointing towards model stability over time as an important consideration</li>
</ul>
<p><img class="smooth center" src="/media/images/articles/2016/ml_not_magic.jpg" /></p>

        </div>
      </div>
      
        <div class="span3 side-box">
          <div style="padding: 8px;">
            <h1 style="margin-bottom: 0; text-align: center;">Popular Articles</h1>
            <style>
            ul.unstyled li { padding-bottom: 6px; }
            </style>
            <ul class="crossed">
              <li><a target="_blank" href="/articles/2016/ml_not_magic.html">It's ML, not magic: the rise of AI-prefix investing</a></li>
              <li><a target="_blank" href="/articles/2016/architectures_are_the_new_feature_engineering.html">In deep learning, architecture engineering is the new feature engineering</a></li>
              <li><a target="_blank" href="/articles/2015/keras_qa.html">Question answering on the Facebook bAbi dataset using recurrent neural networks and 175 lines of Python + Keras</a></li>
              <li><a target="_blank" href="/articles/2015/google_sparsehash.html">How Google Sparsehash achieves two bits of overhead per entry using sparsetable</a></li>
              <li><a target="_blank" href="/articles/2013/where_did_all_the_http_referrers_go.html">Where did all the HTTP referrers go?</a></li>
            </ul>

            <p class="center-text">
              <b>Interested in saying hi? ^_^</b><br />
              <div class="contact-me">
              <a href="https://www.twitter.com/Smerity/" alt="Twitter"><i class="icon-twitter"></i></a><a href="https://facebook.com/smerity" alt="Facebook"><i class="icon-facebook-sign"></i></a><a href="https://github.com/Smerity/" alt="GitHub"><i class="icon-github"></i></a><a href="http://au.linkedin.com/in/smerity" alt="LinkedIn"><i class="icon-linkedin"></i></a><a href="mailto:smerity@smerity.com" alt="Email"><i class="icon-envelope-alt"></i></a>
              </div>
            </p>

            <div class="well well-small">
              <!--<h1 style="margin-bottom: 0; margin-top: 0; text-align: center;">Hi, I'm Smerity</h1>-->
              <div align="center">
                <!--img class="shadowed pronounced" src="/media/images/smerity_bw.jpg" />-->
              </div>
              <!-- TODO: Place Twitter here -->
              <p>
                I'm <b>Stephen Merity</b>, better known in most places as <b>Smerity</b>.
              </p>
              <p>
                <b>MetaMind:</b><br />
                Research scientist, deep learning
              </p>
              <p>
                <b>Harvard University:</b><br />
                <a href="http://www.seas.harvard.edu/programs/graduate/computational-science-and-engineering/master-of-science-in-cse">MS in CSE</a>
              </p>
              <p>
                <b>Sydney University:</b><br />
                <a href="http://sydney.edu.au/engineering/it/current_students/undergrad/bit.shtml">BIT</a> (University Medal + First Class Honours)
              </p>
              <p>
                <b><a href="/abme.html">Full about me</a></b>
              </p>
            </div>

            
          </div>
        </div>
      
      </div>
    
    
<footer>
  You reached the bottom! Reading can be challenging, so I think you deserve a reward. I'd offer you <a href="http://www.minecraftwiki.net/wiki/Cake">cake</a>, but after <a href="http://en.wikipedia.org/wiki/Portal_(video_game)">a certain robot-human conflict</a> started due to cake it may not be a wise choice. So here:
  <div align="center">
    <img src="/media/images/mini_coin.png" height="32" width="32" />
  </div>
  Take a coin. No, not a Bitcoin. Do you know how <a title="When I originally put this note here, Bitcoins were only a few dollars each...">expensive</a> those are? ಠ_ಠ
</footer>

  </div>
  
  
  <link rel="stylesheet" type="text/css" href="/media/css/fontawesome/fontawesome.css" />
  <!--[if lt IE 9]>
    <link rel="stylesheet" type="text/css" href="/media/css/fontawesome/fontawesome-ie7.css" />
  <![endif]-->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,700' rel='stylesheet' type='text/css'>

  
  </body>
</html>